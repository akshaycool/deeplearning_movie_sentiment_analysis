Imp notes
Tutorial link -> https://www.kaggle.com/c/word2vec-nlp-tutorial#part-2-word-vectors



Part1 - Bag of Words model

1) Fetching data from the labeledTrainingData using pandas
2) Clean the data of reviews , because it contains different characters which 
have to be determined if needed to keep or not for sentiment analysis
	a) Beautiful soup library for extracting the important text from tags.
	b) In the obtained text,
		1)to remove the punctuations and numbers -> package re(for regex)
			eg letters_only = re.sub("[^a-zA-Z]"," ",example.get_text())
				ie Find anything that is not a lowercase letter or uppercase
				letter with  " "
	c) Do a text.lower() and split into words(Tokenization)
		lower_case = letters_only.lower()
		words = lower_case.split()
	d) Finally, deal with frequently occuring words which don't carry much meaning -> stop words
		eg a,is,to,etc
3) Creating features using Bag of Words model (scikit learn)
	a)Bag of Words model is basically a vocab matrix containing the words and the count of the word in the
	given sentence.
	b)In the given dataset, the resulting array -> 25000 x 5000
4) We use RandomForestClassifier to classfy 

	Random Forest Classifier
	1) Large collection of decorrelated decision trees
	2) Given sample set is sub sampled and independent decision trees are created and 
	rank the classifier , and majority of count of classifying label is selected the winner.

5) Representations of Text(Concept)
Refer => https://docs.google.com/file/d/0B7XkCwpI5KDYRWRnd1RzWXQ2TWc/edit
	A)Local Representations
		a)N-grams
		b)Bag-of-words
		c)1-of-N-coding
	B)Continuous Representations
		a)Latent Semantic Analysis
		b)Latent Dirichlet Allocation
		c)Distributed Representions



Part2 - Word2Vec Google library
First 2 steps are same as the above with the only exception being stopwords removal has to be optional in the cleaning process
because word2vec is more accurate with contexual sense of stopwords

1)The input format for wordvec is list of words for each sentences ie basically list of lists.
2) Here, the challenge is to split paragraph into sentences because of the different punctuation which can be used like ?,!,. 
and so on.
3) We use NLTK's punkt tokenizer for sentence splitting.
4)Using the gemsim library for training word2vec model
	a)The vocabulary generated contains list of sentences which is list of respective words(train+untrain set)
	b) Set the features to train the model like 
		i)	min_word_count ie min times the word has to occur,
		ii)	downsampling size -> it's the number which denotes how many words can be left out of vocab ,ensuring
		the minimum word count
5)Now we have the model trained, we need to pass in the RandomForestClassifier, however the features need to be 
set
	a)The algo followed for preparing feature vector is 
	 	Given a set of reviews (each one a list of words), calculate 
	 	# the average feature vector for each one and return a 2D numpy array
6)Final step - prediction - results -> 0.83 vs 0.84(bag_of_words model)



















